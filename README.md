# Image Captioning Using Attention Mechanism on COCO Dataset

## Overview

This project implements an image captioning model using an attention mechanism on the [COCO 2017 Dataset](https://cocodataset.org/#home). The goal of the model is to generate textual descriptions (captions) for images. The attention mechanism allows the model to focus on specific parts of an image while generating each word of the caption, mimicking how humans describe scenes.

The project uses the following key components:
- **Convolutional Neural Networks (CNN)** for image feature extraction.
- **Long Short-Term Memory (LSTM) networks** for sequence generation (text generation).
- **Attention Mechanism** to improve caption generation by dynamically focusing on different image regions.
- **COCO 2017 Dataset** for training and validation.

## Dataset

- **Dataset:** [COCO 2017](https://cocodataset.org/#download)
- **Annotations:** COCO provides captions for the images in JSON format (`captions_train2017.json`).
- **Images:** The images are stored in the `train2017` folder and are accessed by their IDs.

## Key Components

### 1. Preprocessing
- **Text Cleaning:** Includes removing punctuations, numbers, and single characters from captions to prepare them for training.
- **Tokenizer:** The captions are tokenized using Keras' `Tokenizer` to convert text to sequences.
  
### 2. Image Feature Extraction
- **VGG16** is used as the base CNN model to extract image features. The features are extracted from the fully connected layers after pre-processing the images using the `preprocess_input` function.

### 3. Caption Generation Model
- **LSTM Model:** The core of the model is based on an LSTM network to handle sequence generation.
- **Attention Mechanism:** This is applied to dynamically focus on parts of the image when predicting each word in the sequence.

### 4. Training Process
- The dataset is split into training and testing sets using `train_test_split`.
- The model is trained using `Categorical Cross-Entropy Loss` and optimized using the `Adam Optimizer`.
- **Model Checkpoints** are used to save the best performing model during training.

## Code Structure

### 1. Preprocessing
- **Text Cleaning Functions:**
  - `remove_punctuation()`: Removes punctuations from the captions.
  - `remove_single_character()`: Removes single characters from the text.
  - `remove_numeric()`: Filters out numeric values from captions.

- **Tokenization:** The captions are tokenized, padded, and converted into sequences suitable for LSTM input.

### 2. Data Loading
- **Loading COCO Captions:** 
  - The annotations file `captions_train2017.json` is loaded, and captions are paired with image IDs.
  - Image paths are generated using the image IDs, and the dataset is constructed as a pandas DataFrame with image paths and captions.

### 3. Model Architecture
- **CNN for Image Features (VGG16):** Pre-trained VGG16 model is used to extract image features.
- **LSTM for Captioning:** A sequence of LSTM layers is used to generate captions.
- **Attention Mechanism:** This layer focuses on relevant parts of the image when generating words in the caption.
  
### 4. Model Training and Evaluation
- **Training:** The model is trained on the training set with batch processing and checkpoints to save the best models.
- **Evaluation:** The model's performance is evaluated using test data.

### 5. Visualization
- **Image Captioning:** The notebook also provides functionality to visualize the model's predictions by generating captions for randomly selected images and comparing the generated captions with ground truth.

## Instructions to Run

1. **Install Required Libraries:**
   - Install the dependencies using pip:
     ```bash
     pip install -r requirements.txt
     ```

2. **Dataset Preparation:**
   - Download the COCO dataset and place the images and captions file in the appropriate directories as specified in the notebook.

3. **Running the Notebook:**
   - Load the Jupyter notebook and run the cells step by step to preprocess the data, train the model, and evaluate it.

4. **Training the Model:**
   - The notebook will automatically train the model and save the best model checkpoints during the training process.

5. **Generate Captions:**
   - Once the model is trained, you can visualize the captions generated by the model for any image in the COCO dataset.

## Results

- The model is evaluated using loss values during training, and captions are generated for test images to visually assess the model's performance.
- Sample predictions are provided in the notebook, comparing ground truth captions with model-generated captions.

## Dependencies

- Python 3.x
- TensorFlow/Keras
- numpy
- pandas
- matplotlib
- scikit-learn
- PIL (Pillow)

## Acknowledgements

- This project uses the COCO dataset, a large-scale object detection, segmentation, and captioning dataset.
- The image features are extracted using a pre-trained VGG16 network from Keras' model zoo.
